{"cells":[{"cell_type":"markdown","metadata":{"id":"xU_5L2yax2p7"},"source":["# VS Code Extension for Code Commenting\n","<img src=\"https://i.ytimg.com/vi/WHwi6-hzJhA/maxresdefault.jpg\">"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21337,"status":"ok","timestamp":1652987110265,"user":{"displayName":"uvbro business","userId":"06149407597772973103"},"user_tz":-330},"id":"_3mjfQWW-4TP","outputId":"2fcde512-eb7f-4668-f2f4-5aef93c6ac4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mounting the gdrive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6314,"status":"ok","timestamp":1652987116573,"user":{"displayName":"uvbro business","userId":"06149407597772973103"},"user_tz":-330},"id":"nMYyR89jBclk","outputId":"f759d4c1-0327-4fc7-e2c7-5b36198d50a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fastapi\n","  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n","\u001b[?25l\r\u001b[K     |██████                          | 10 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 44.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (1.5.5)\n","Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 28.6 MB/s \n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s \n","\u001b[?25hCollecting starlette==0.19.1\n","  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n","\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n","  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n","\u001b[K     |████████████████████████████████| 11.1 MB 45.0 MB/s \n","\u001b[?25hCollecting anyio<5,>=3.4.0\n","  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 11.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.2.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n","Collecting sniffio>=1.1\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n","Collecting asgiref>=3.4.0\n","  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n","Collecting h11>=0.8\n","  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=0bbcb5bab388621bc72c9f090495ee5eea0b6b8a3f467f5b3cf89d3844dd96cd\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: sniffio, anyio, starlette, pydantic, h11, asgiref, uvicorn, pyngrok, fastapi\n","Successfully installed anyio-3.6.1 asgiref-3.5.2 fastapi-0.78.0 h11-0.13.0 pydantic-1.9.1 pyngrok-5.1.0 sniffio-1.2.0 starlette-0.19.1 uvicorn-0.17.6\n"]}],"source":["!pip install fastapi nest-asyncio pyngrok uvicorn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJIuI90rnuS4"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqbwx-oknyYH"},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFRVmdgJnzKb"},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IQLf7D4n3HR"},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSe8ucavn8S_"},"outputs":[],"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","vocab_size = 150000 \n","sequence_length = 20 \n","batch_size = 64\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")\n","\n","transformer.load_weights(\"/content/drive/MyDrive/Trained Model/For epoch 50-61% accuracy/check/weights-06-0.34.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gkaapo0LoHaS"},"outputs":[],"source":["!cp \"/content/drive/MyDrive/Trained Model/For epoch 30, 52% accuarcy/V_cmt.zip\" \"/content/\"\n","!cp \"/content/drive/MyDrive/Trained Model/For epoch 30, 52% accuarcy/V_code.zip\" \"/content/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1652987131505,"user":{"displayName":"uvbro business","userId":"06149407597772973103"},"user_tz":-330},"id":"l_EVDxyRoKd4","outputId":"8e247bdf-d287-46f9-b80a-a6da48aa8463"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/V_cmt.zip\n","   creating: content/vectorizer_cmt/\n","  inflating: content/vectorizer_cmt/keras_metadata.pb  \n","   creating: content/vectorizer_cmt/assets/\n","   creating: content/vectorizer_cmt/variables/\n","  inflating: content/vectorizer_cmt/variables/variables.data-00000-of-00001  \n","  inflating: content/vectorizer_cmt/variables/variables.index  \n","  inflating: content/vectorizer_cmt/saved_model.pb  \n","Archive:  /content/V_code.zip\n","   creating: content/vectorizer_code/\n","  inflating: content/vectorizer_code/keras_metadata.pb  \n","   creating: content/vectorizer_code/assets/\n","   creating: content/vectorizer_code/variables/\n","  inflating: content/vectorizer_code/variables/variables.data-00000-of-00001  \n","  inflating: content/vectorizer_code/variables/variables.index  \n","  inflating: content/vectorizer_code/saved_model.pb  \n"]}],"source":["!unzip /content/V_cmt.zip\n","!unzip /content/V_code.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1258,"status":"ok","timestamp":1652987132761,"user":{"displayName":"uvbro business","userId":"06149407597772973103"},"user_tz":-330},"id":"ZPKL2eZhoQPH","outputId":"b11dc866-e995-4a0c-924a-fa0e76a40869"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["loaded_vectorize_layer_model_1 = tf.keras.models.load_model(\"/content/content/vectorizer_cmt\")\n","cmt_vectorization = loaded_vectorize_layer_model_1.layers[0]\n","\n","loaded_vectorize_layer_model_1 = tf.keras.models.load_model(\"/content/content/vectorizer_code\")\n","code_vectorization = loaded_vectorize_layer_model_1.layers[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuDC1TLvoVr5"},"outputs":[],"source":["cmt_vocab = cmt_vectorization.get_vocabulary()\n","cmt_index_lookup = dict(zip(range(len(cmt_vocab)), cmt_vocab))\n","max_decoded_sentence_length = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLuF2y3loYSY"},"outputs":[],"source":["def decode_sequence(input_sentence):\n","    input_sentence=input_sentence.lower()\n","    tokenized_input_sentence = code_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = cmt_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = cmt_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yscMVhicodQI"},"outputs":[],"source":["def final_clean_out(Comment):\n","  claen_cmt=[]\n","  cmt_lst=Comment.split(\" \")\n","  cmt_lst=[i for i in cmt_lst if i !=\"\"]\n","  for i in range(1,len(cmt_lst)):\n","    if cmt_lst[i] != cmt_lst[i-1]:\n","      claen_cmt.append(cmt_lst[i])\n","    if cmt_lst[i]=='end':\n","      break\n","\n","  final_out=\" \".join(claen_cmt[0:-1])\n","  return final_out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8_7IxmI_5hZ","outputId":"a621db7e-5efe-428b-9f6b-b9ba864d8f80"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Public URL: NgrokTunnel: \"http://7529-35-204-93-204.ngrok.io\" -> \"http://localhost:80\"\n"]}],"source":["import nest_asyncio\n","import uvicorn\n","\n","from fastapi import FastAPI\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel\n","from pyngrok import ngrok\n","\n","class Message(BaseModel):\n","    code: str\n","\n","app = FastAPI()\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=['*'],\n","    allow_credentials=True,\n","    allow_methods=['*'],\n","    allow_headers=['*'],\n",")\n","\n","@app.post('/')\n","async def root(message: Message):\n","    cmt = final_clean_out(decode_sequence((message.code)))\n","    return cmt\n","\n","url = ngrok.connect(port=80)\n","print('Public URL:', url)\n","nest_asyncio.apply()\n","uvicorn.run(app, port=80)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"VS_Code_Back_end.ipynb","provenance":[],"authorship_tag":"ABX9TyMX3tmqxXVoWUQxouQWeFan"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}